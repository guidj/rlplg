"""
Tower of Hanoi.
https://en.wikipedia.org/wiki/Tower_of_Hanoi

There are three pegs, and a given number of disks - three or four.
The game starts with the disks stacked on the first peg,
sorted by size with the smallest disk on top.
The game ends when all disks are stacked on the last peg,
sorted by size with the smallest disk on top.

Rules:
1. Only one disk can be moved at a time
2. A legal move consitutes moving the top most disk
from its current peg onto another
3. No disk can be placed on top of a smaller disk

Since there are three pegs, there are only six possible
moves: moving the top most disk from peg 1 to either
2 or 3; from peg 2 to either 1 or 3, and so on.

The number of moves required to solve the puzzle is
is 2^{n} - 1, where n is the number of disks.

Code based on https://github.com/xadahiya/toh-gym.
"""

import copy
import functools
from typing import Any, Callable, List, Mapping, Optional, Sequence, Tuple

import gymnasium as gym
import numpy as np
from gymnasium import spaces

from rlplg import combinatorics, core, npsci
from rlplg.core import InitState, MutableEnvTransition, RenderType, TimeStep

ENV_NAME = "TowerOfHanoi"
ACTIONS = [(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]
NUM_PEGS = 3
OBS_KEY_STATE = "state"
OBS_KEY_NUM_PEGS = "num_pegs"
MIN_DISKS = 1
MAX_DISKS = 9


class TowerOfHanoi(gym.Env[Mapping[str, Any], int]):
    """
    An environment where an agent is meant to follow a pre-defined
    sequence of actions.
    """

    def __init__(self, num_disks: int, render_mode: str = "rgb_array"):
        """
        Args:
            disks: number of disks in the game.
        """
        super().__init__()
        if num_disks < MIN_DISKS or num_disks > MAX_DISKS:
            raise ValueError(f"Disks must be between [1, 9]. Got: {num_disks}")

        self.num_disks = num_disks
        self.num_pegs = NUM_PEGS
        self.render_mode = render_mode
        self.action_space = spaces.Discrete(len(ACTIONS))
        self.observation_space = spaces.Dict(
            {
                OBS_KEY_NUM_PEGS: spaces.Box(
                    low=self.num_pegs, high=self.num_pegs, dtype=np.int64
                ),
                OBS_KEY_STATE: spaces.Tuple(
                    [spaces.Discrete(self.num_pegs) for _ in range(self.num_disks)]
                ),
            }
        )

        self.transition: MutableEnvTransition = {}
        num_states = self.num_pegs**self.num_disks
        terminal_state = num_states - 1
        state_id_to_state = create_state_id_to_state_fn(
            num_pegs=self.num_pegs, num_disks=self.num_disks
        )
        for state_id in range(num_states):
            self.transition[state_id] = {}
            # generate state
            state_obs = state_id_to_state(state_id)
            for action in range(len(ACTIONS)):
                self.transition[state_id][action] = []
                actual_next_state_obs, actual_reward = apply_action(
                    observation=state_obs, action=action
                )
                actual_next_state_id = get_state_id(actual_next_state_obs)
                for next_state_id in range(num_states):
                    prob = 1.0 if next_state_id == actual_next_state_id else 0.0
                    reward = (
                        actual_reward if next_state_id == actual_next_state_id else 0.0
                    )
                    terminated = (
                        state_id != next_state_id and next_state_id == terminal_state
                    )
                    self.transition[state_id][action].append(
                        (prob, next_state_id, reward, terminated)
                    )

        # env specific
        self._observation: Mapping[str, Any] = {}
        self._seed: Optional[int] = None

    def step(self, action: int) -> TimeStep:
        """Updates the environment according to action and returns a `TimeStep`.

        See `step(self, action)` docstring for more details.

        Args:
            action: A policy's chosen action.
        """
        if self._observation == {}:
            raise RuntimeError(
                f"{type(self).__name__} environment needs to be reset. Call the `reset` method."
            )
        new_observation, reward = apply_action(self._observation, action)
        finished = is_terminal_state(new_observation)
        self._observation = new_observation
        return copy.copy(self._observation), reward, finished, False, {}

    def reset(
        self, *, seed: Optional[int] = None, options: Optional[Mapping[str, Any]] = None
    ) -> InitState:
        """Starts a new sequence, returns the `InitState` for this environment.

        See `reset(self)` docstring for more details
        """
        del options
        self.seed(seed)
        self._observation = state_observation(
            num_pegs=self.num_pegs, state=(0,) * self.num_disks
        )
        return copy.copy(self._observation), {}

    def render(self) -> RenderType:
        """
        Renders a view of the environment's current
        state.
        """
        if self._observation == {}:
            raise RuntimeError(
                f"{type(self).__name__} environment needs to be reset. Call the `reset` method."
            )
        if self.render_mode == "rgb_array":
            return np.array(self._observation[OBS_KEY_STATE])
        return super().render()

    def seed(self, seed: Optional[int] = None) -> Any:
        """
        Sets a seed, if defined.
        """
        if seed is not None:
            self._seed = seed
            np.random.seed(seed)
        return self._seed


class TowerOfHanoiMdpDiscretizer(core.MdpDiscretizer):
    """
    Creates an environment discrete maps for states and actions.
    """

    def state(self, observation: Mapping[str, Any]) -> int:
        """
        Maps an observation to a state ID.
        """
        del self
        return get_state_id(observation)

    def action(self, action: int) -> int:
        """
        Maps an agent action to an action ID.
        """
        del self
        action_: int = npsci.item(action)
        return action_


def apply_action(observation: Mapping[str, Any], action: int) -> Tuple[Any, float]:
    """
    Computes a new observation and reward given the current state and action.

    Example:
        disks: 3
        starting_state: (0, 0, 0)
        possible_states: (0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0)... (2, 2, 2)

        S = (0, 0, 0)
            Try to move the second disk to the last peg.
            Nothing changes, since there are disks on top of it.
            Penalty for moving and for an invalid move.
            reward = -1 -1 = -2
        S = (0, 0, 0)
            Move the top disk from the first peg to the second.
            Penalty for moving.
            reward = -1
        S = (1, 0, 0)
            Move the top disk from the first peg to the third.
            Penalty for moving.
            reward = -1
        S = (1, 2, 0)
        ....
        S = (1, 2, 2)
            Move the top disk from the second peg to the third.
            Penalty for moving. Terminal state
            reward = -1
        S = (2, 2, 2)
            Try to move the top disk from the last peg to the first.
            Nothing changes.
            reward = 0
    """
    state = observation[OBS_KEY_STATE]
    num_disks = len(state)
    new_observation = dict(observation)
    source, dest = ACTIONS[action]

    # if the top disk on any peg isn't the smallest
    # the state is invalid.
    pegs: Mapping[int, List[int]] = {
        peg: [] for peg in range(observation[OBS_KEY_NUM_PEGS])
    }
    for idx in range(num_disks):
        pegs[state[idx]].append(idx)

    for peg, peg_disks in pegs.items():
        if peg_disks != sorted(peg_disks):
            raise ValueError(f"Invalid state. Peg {peg} is invalid: {peg_disks}")

    # In terminal state already, nothing changes
    if is_terminal_state(observation):
        move_penalty = 0.0
        reward = 0.0

    # No disk to move or dest has smaller disk on top
    elif len(pegs[source]) == 0 or (pegs[dest] and pegs[source][0] > pegs[dest][0]):
        move_penalty = -1.0
        reward = -1.0

    # Ok to move
    else:
        move_penalty = -1.0
        reward = 0.0
        # move top disk from source to dest
        mutable_state = list(state)
        mutable_state[pegs[source][0]] = dest
        new_observation[OBS_KEY_STATE] = tuple(mutable_state)
    return new_observation, move_penalty + reward


def is_terminal_state(observation: Mapping[str, Any]) -> bool:
    """
    Determines if the given state is terminal.
    """
    # does that fact that we just went into the
    # terminal state matter? No
    state = observation[OBS_KEY_STATE]
    terminal_state = (observation[OBS_KEY_NUM_PEGS] - 1,) * len(state)
    return state == terminal_state  # type: ignore


def create_env_spec(num_disks: int) -> core.EnvSpec:
    """
    Creates an env spec from a config.
    """
    environment = TowerOfHanoi(num_disks=num_disks)
    discretizer = TowerOfHanoiMdpDiscretizer()
    num_states = environment.num_pegs**num_disks
    num_actions = len(ACTIONS)
    return core.EnvSpec(
        name=ENV_NAME,
        level=str(num_disks),
        environment=environment,
        discretizer=discretizer,
        mdp=core.EnvMdp(
            env_desc=core.EnvDesc(num_states=num_states, num_actions=num_actions),
            transition=environment.transition,
        ),
    )


def get_state_id(observation: Mapping[str, Any]) -> int:
    """
    Computes an integer Id that represents that state.
    """
    return combinatorics.sequence_to_integer(
        space_size=observation[OBS_KEY_NUM_PEGS], sequence=observation[OBS_KEY_STATE]
    )


def create_state_id_to_state_fn(
    num_pegs: int, num_disks: int, cache_size: int = 2**7
) -> Callable[[int], Mapping[str, Any]]:
    @functools.lru_cache(maxsize=cache_size)
    def state_id_to_state(state_id: int) -> Mapping[str, Any]:
        """
        Generates a unique Id for a given state.
        """
        return state_observation(
            num_pegs=num_pegs,
            state=combinatorics.interger_to_sequence(
                space_size=num_pegs, sequence_length=num_disks, index=state_id
            ),
        )

    return state_id_to_state


def state_observation(num_pegs: int, state: Sequence[int]) -> Mapping[str, Any]:
    """
    Generates a state observation, given an interger Id and the number
    of disks.

    Args:
        num_peges: the number of pegs in the game.
        state: a sequence defining the placement of disks.

    Returns:
        An observation of the environment.
    """
    return {
        OBS_KEY_NUM_PEGS: num_pegs,
        OBS_KEY_STATE: state,
    }
